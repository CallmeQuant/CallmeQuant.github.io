I"6É<p>Here, we discuss and visualize the mode-seeking behavior of the reverse KL divergence.</p>

<h2 id="introduction">Introduction</h2>

<p>Recall that the reverse KL divergence between two distributions, $q(x)$ and $p(x)$ is defined as</p>

\[D_{KL}(q(x) \| p(x)) = \mathbb{E}_{q(x)}\left[ \log \frac{q(x)}{p(x)} \right] = \int_{\mathbb{X}} \log \frac{q(x)}{p(x)} q(x) dx.\]

<p>Expanding the log, we can write this in terms of an entropy term and a cross entropy term</p>

<p>\begin{align} &amp;\mathbb{E}_{q(x)}\left[ \log q(x) \right] - \mathbb{E}_{q(x)}\left[ \log p(x) \right] \\ =&amp; -H(q) + H_c(q, p) \end{align}</p>

<p>If we can estimate the entropy of $q$ exactly, then all that‚Äôs left is to get a Monte Carlo estimate of $\mathbb{E}_{q(x)}\left[ \log p(x) \right]$. We can do this by sampling $x_1, \dots, x_T \sim q(x)$ and computing</p>

\[\frac1T\sum\limits_{t=1}^T \log p(x_t).\]

<h2 id="mode-seeking-behavior">Mode-seeking behavior</h2>

<p>There are two types of KL divergence: forward and reverse. Forward KL divergence corresponds to $D_{KL}(p|q)$, and reverse corresponds to $D_{KL}(q|p)$.</p>

<p>The reverse KL divergence is said to be ‚Äúmode-seeking‚Äù. This means that the divergence will be low when $q$ places density only where $p$ places density, and the divergence will be high when $q$ places density where $p$ does not. In other words, if $p$ is multimodal, the divergence will ‚Äúprefer‚Äù or ‚Äúseek‚Äù orientations of $q$ that only place mass in one of the modes, rather than spreading out the mass between all of $p$‚Äôs modes.</p>

<p>For example, let $p(x)$ be a mixture of two univariate Gaussians,</p>

\[p(x) = \pi\mathcal{N}(\mu_1, \sigma^2_1) + (1-\pi)\mathcal{N}(\mu_2, \sigma^2_2)\]

<p>where $\pi$ controls the mixing proportions. Below is a plot of their densities where we set $\mu_1=-4, \mu_2=4$, and $\sigma^2_1=\sigma^2_2=1$.</p>

<p align="center">
  <img src="/assets/klqp_univariate_densities.png" />
</p>

<p>Suppose we approximate $p(x)$ with a univariate Gaussian $q(x)$ by minimizing the KL divergence with respect to $q$‚Äôs variational parameters. We find that the KL divergence is lower when $q$ is centered around one of $p$‚Äôs modes, rather than spread between them. The figure below shows this for three possible orientations of $q$.</p>

<p align="center">
  <img src="/assets/klqp_univariate_densities_divergences.png" />
</p>

<p>This mode-seeking behavior of the reverse KL divergence can be intuited from its form above. When $q$ is low but $p$ is high, then the divergence is low. However, if $q$ is high but $p$ is low, then the divergence is large.</p>

<h2 id="visualization">Visualization</h2>

<p>Next, we show a visualization of optimization using the reverse KL divergence. Consider data $x$ that is distributed according to a mixture of two-dimensional Gaussians,</p>

\[p(x) = \pi \mathcal{N}_2(\mu_1, \Sigma_1) + (1-\pi) \mathcal{N}_2(\mu_2, \Sigma_2).\]

<p>This density is clearly multimodal, assuming $\mu_1 \neq \mu_2$.</p>

<p>Suppose we would like to approximate $p(x)$ with a single, unimodal Gaussian,</p>

\[q(x) = \mathcal{N}(\lambda_{\mu}, \lambda_{\Sigma})\]

<p>where $\lambda_{\mu}$ is the variational mean, and $\lambda_{\Sigma}$ is the variational covariance.</p>

<p>Further, assume that we make a mean-field approximation, meaning that each dimension of the variational approximation is independent of the others. More concisely, we assume</p>

\[\lambda_{\Sigma} = \text{diag}(\sigma^2_1, \sigma^2_2).\]

<p>In the experiment below, we assume the following:</p>

\[\mu_1 = \begin{bmatrix} -4 \\ -4 \end{bmatrix}, \mu_2 = \begin{bmatrix} 4 \\ 4 \end{bmatrix}, \Sigma_1 = \Sigma_2 = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}.\]

<p>Below is a contour plot of the density that we wish to approximate.</p>

<p align="center">
  <img src="/assets/target_density_klqp.png" width="500" />
</p>

<p>We optimize $\lambda_{\mu}, \lambda_{\Sigma}$ by minimizing the KL-qp divergence. In this post, we initialize the variational parameters as</p>

\[\lambda_{\mu} =\begin{bmatrix} 0 \\ 0 \end{bmatrix}, \lambda_{\Sigma} = I_2.\]

<p>Below, we do this optimization using gradient descent and the Adam optimizer. We can visualize the optimization process by plotting the contours of the true density and the variational density over the iterations. The GIF below shows this visualization for our mixture of Gaussians example. The variational density is shown in the reddish colors.</p>

<p align="center">
  <img src="/assets/2d_gaussian_klqp1.gif" width="700" />
</p>

<p>As we can see, the variational density immediately moves to one of the modes (in this case, the mode with mean $\begin{bmatrix} 2 &amp; 2 \end{bmatrix}^\top$).</p>

<h2 id="code">Code</h2>

<p>Below is the code for running the 2D Gaussian experiment above. It also creates the contour visualization. This is based on the examples in the <a href="https://github.com/HIPS/autograd">autograd repo</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value_and_grad</span>
<span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">autograd.scipy.stats</span> <span class="kn">import</span> <span class="n">t</span><span class="p">,</span> <span class="n">norm</span><span class="p">,</span> <span class="n">multivariate_normal</span>
<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">loggamma</span>
<span class="kn">from</span> <span class="nn">autograd.core</span> <span class="kn">import</span> <span class="n">getval</span>
<span class="kn">from</span> <span class="nn">autograd.numpy</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">npr</span>
<span class="kn">from</span> <span class="nn">autograd.misc.optimizers</span> <span class="kn">import</span> <span class="n">adam</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span> <span class="k">as</span> <span class="n">mvn</span>
<span class="kn">from</span> <span class="nn">autograd.scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span> <span class="nn">autograd.scipy.stats.t</span> <span class="k">as</span> <span class="n">t_dist</span>

<span class="kn">import</span> <span class="nn">matplotlib.animation</span> <span class="k">as</span> <span class="n">animation</span>
<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="n">mpimg</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">autograd.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span>


<span class="k">class</span> <span class="nc">Model</span><span class="p">():</span>

	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">cov</span><span class="p">):</span>

		<span class="bp">self</span><span class="p">.</span><span class="n">mean1</span> <span class="o">=</span> <span class="n">mean1</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">mean2</span> <span class="o">=</span> <span class="n">mean2</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">cov</span> <span class="o">=</span> <span class="n">cov</span>
		<span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="k">assert</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean2</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>


	<span class="k">def</span> <span class="nf">log_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">zs</span><span class="p">):</span>
		
		<span class="n">clust1_density</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean1</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">)</span>
		<span class="n">clust2_density</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean2</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="n">clust1_density</span><span class="p">,</span> <span class="n">clust2_density</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
		<span class="n">num_clust1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
		<span class="n">num_clust2</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">num_clust1</span>
		<span class="n">samples_mode1</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">mean1</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_clust1</span><span class="p">)</span>
		<span class="n">samples_mode2</span> <span class="o">=</span> <span class="n">mvn</span><span class="p">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">mean2</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_clust2</span><span class="p">)</span>
		<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">samples_mode1</span><span class="p">,</span> <span class="n">samples_mode2</span><span class="p">])</span>
		<span class="k">return</span> <span class="n">samples</span>


<span class="k">class</span> <span class="nc">ApproxMFGaussian</span><span class="p">():</span>

	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">pass</span>

	<span class="k">def</span> <span class="nf">log_density</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_param</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
		<span class="c1"># variational density evaluated at samples
</span>		<span class="k">return</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">var_param</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">var_param</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)))</span>

	<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_param</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
		<span class="n">stddevs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">var_param</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
		<span class="k">return</span> <span class="n">var_param</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">seed</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">stddevs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">gradq</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_param</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
		<span class="n">objective</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">vparam</span> <span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_density</span><span class="p">(</span><span class="n">vparam</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
		<span class="n">grad_q</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">objective</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">grad_q</span><span class="p">(</span><span class="n">var_param</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">var_param</span><span class="p">):</span>
		<span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">var_param</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
		
<span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Generate data
</span><span class="n">mean1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">])</span>
<span class="n">mean2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">mean1</span><span class="o">=</span><span class="n">mean1</span><span class="p">,</span> <span class="n">mean2</span><span class="o">=</span><span class="n">mean2</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">)</span>
<span class="n">approx</span> <span class="o">=</span> <span class="n">ApproxMFGaussian</span><span class="p">()</span>

<span class="n">S</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Variational parameters: first d elements are the mean, last d elements are the diagonal of log-covariance
</span><span class="n">variational_mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">variational_log_cov_diag</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">variational_param</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">variational_mean</span><span class="p">,</span> <span class="n">variational_log_cov_diag</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">variational_param</span><span class="p">,</span> <span class="nb">iter</span><span class="p">):</span>

	<span class="n">samples</span> <span class="o">=</span> <span class="n">approx</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">variational_param</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>

	<span class="n">lik</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">log_density</span><span class="p">(</span><span class="n">samples</span><span class="p">))</span>
	<span class="n">entropy</span> <span class="o">=</span> <span class="n">approx</span><span class="p">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">variational_param</span><span class="p">)</span>
	<span class="n">elbo</span> <span class="o">=</span> <span class="n">lik</span> <span class="o">+</span> <span class="n">entropy</span>

	<span class="k">return</span> <span class="o">-</span><span class="n">elbo</span> <span class="o">/</span> <span class="n">S</span>

<span class="n">seed</span> <span class="o">=</span> <span class="n">npr</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_isocontours</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">xlimits</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">ylimits</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="n">numticks</span><span class="o">=</span><span class="mi">101</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">''</span><span class="p">):</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">xlimits</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">numticks</span><span class="p">)</span>
	<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">ylimits</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">numticks</span><span class="p">)</span>
	<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
	<span class="n">zs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">ravel</span><span class="p">()),</span> <span class="n">np</span><span class="p">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">ravel</span><span class="p">())]).</span><span class="n">T</span><span class="p">)</span>
	<span class="n">Z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
	<span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([])</span>
	<span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'white'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ion</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">target_distribution</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">log_density</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plot_isocontours</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">target_distribution</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"p(z)"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">facecolor</span><span class="o">=</span><span class="s">'white'</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ion</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">(</span><span class="n">block</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">imgs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"     Epoch     |    Objective     |    Param vals"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">print_perf</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">iter</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
	
	<span class="n">bound</span> <span class="o">=</span> <span class="n">objective</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span>
	<span class="n">message</span> <span class="o">=</span> <span class="s">"{:15}|{:20}|{:15}|{:15}{:15}|{:15}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">iter</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">bound</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mi">2</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">2</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="mi">2</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="mi">2</span><span class="p">))</span>
	<span class="k">print</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>

	<span class="n">plt</span><span class="p">.</span><span class="n">cla</span><span class="p">()</span>
	<span class="n">target_distribution</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">log_density</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
	<span class="n">plot_isocontours</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">target_distribution</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"p(z)"</span><span class="p">)</span>

	<span class="n">variational_contour</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mvn</span><span class="p">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">[:</span><span class="mi">2</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">2</span><span class="p">:])))</span>
	<span class="n">plot_isocontours</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">variational_contour</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">'plasma'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"q(z)"</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">step_size</span> <span class="o">=</span> <span class="p">.</span><span class="mi">1</span>

<span class="n">optimized_params</span> <span class="o">=</span> <span class="n">adam</span><span class="p">(</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">(</span><span class="n">objective</span><span class="p">),</span> <span class="n">x0</span><span class="o">=</span><span class="n">variational_param</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span> <span class="n">num_iters</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">print_perf</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="references">References</h2>
<ul>
  <li>Duvenaud, David, and Ryan P. Adams. ‚ÄúBlack-box stochastic variational inference in five lines of Python.‚Äù NIPS Workshop on Black-box Learning and Inference. 2015.</li>
  <li>Huggins, Jonathan, et al. ‚ÄúValidated variational inference via practical posterior error bounds.‚Äù International Conference on Artificial Intelligence and Statistics. PMLR, 2020.</li>
  <li>My code structure is largely based off of the <a href="https://github.com/jhuggins/viabel">code for viabel</a> (i.e., the above paper).</li>
</ul>
:ET