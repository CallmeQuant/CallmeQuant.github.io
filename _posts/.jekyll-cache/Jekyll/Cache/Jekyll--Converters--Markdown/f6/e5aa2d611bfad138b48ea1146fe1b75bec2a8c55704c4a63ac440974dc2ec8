I"Á<p>Maximum likelihood estimation (MLE) is one of the most popular and well-studied methods for creating statistical estimators. This post will review conditions under which the MLE is consistent.</p>

<h2 id="mle">MLE</h2>

<p>Maximum likelihood estimation is a broad class of methods for estimating the parameters of a statistical model. In particular, as its name suggests, its main goal is to find the parameter values that make the data ‚Äúmost likely‚Äù under some assumed family of densities.</p>

<p>Let $X_1, \dots, X_n$ be some data that were generated from an assumed family of densities $f(X_i, \theta)$. The likelihood of the data $L$ for some value of $\theta$ is then just the product of the density evaluated at each of the samples:</p>

\[L = \prod\limits_{i = 1}^n f(X_i, \theta).\]

<p>Often the log-likelihood is more convenient to work with, so we‚Äôll often write the log-likelihood $\ell_n$ as</p>

\[\ell_n(\theta) = \log \left(\prod\limits_{i = 1}^n f(X_i, \theta).\right) = \sum\limits_{i = 1}^n \log f(X_i, \theta).\]

<p>Assume that the data were generated under some true parameter $\theta_0$. Then, the goal of estimation is to use the data to find a guess of the parameter value, $\hat{\theta}$, that is close to $\theta_0$.</p>

<p>The MLE finds the value $\hat{\theta}$ such that:</p>

\[\hat{\theta}_n = \text{arg}\max_{\theta \in \Theta} \ell_n(\theta).\]

<h2 id="consistency">Consistency</h2>

<p>An estimator $\hat{\theta}$ is <strong>consistent</strong> if it approaches the true value $\theta_0$ as more data is observed. In particular, a consistent estimator $\hat{\theta}$ converges in probability to $\theta_0$. That is, for all $\epsilon &gt; 0$,</p>

\[\lim_{n \to \infty} \mathbb{P} \left[ |\hat{\theta}_n - \theta_0|  &gt; \epsilon \right] = 0.\]

<h2 id="consistency-of-mle">Consistency of MLE</h2>

<p>The MLE is consistent when particular conditions are met:</p>

<ol>
  <li>The model must be identifiable.</li>
  <li>The parameter space $\Theta$ must be compact.</li>
  <li>The density function must be continuous.</li>
  <li>The log-likelihood must converge uniformly:</li>
</ol>

\[\sup_{\theta \in \Theta} ||\ell_n(\theta) - \ell(\theta)|| \to_p 0\]

<p>where $\ell(\theta)$ is the expected log-likelihood, $\ell(\theta) = \mathbb{E}[\log f(X_i, \theta)].$</p>

<h2 id="uniform-convergence-condition">Uniform convergence condition</h2>

<p>Let‚Äôs dig a bit deeper on condition (4). Recall that our MLE estimator $\hat{\theta}_n$ is taking the maximum of a function that is based on $n$ samples. As $n$ increases, the function changes, and so does the maximum value. In order to show consistency, we need to keep track of the limiting behavior of both the function and its maximum.</p>

<p>The primary reason that this condition is necessary is so that the <strong>maximum of the limiting function</strong> and the <strong>limit of the maximum of these functions</strong> are the same.</p>

<p>This jargon can be pretty confusing, so let‚Äôs look at an example (credit to Prof. Matias Cattaneo for this example; see references) where this uniform convergence breaks down.</p>

<p>Assume we have a likelihood function that looks like the plot below, where the true parameter value $\theta_0 = 5$. This is the likelihood function for $1$ sample. Already, we can tell that the MLE will be incorrect in this example: there are two peaks, and the one around $\hat{\theta} = 2$ is higher.</p>

<p><img src="/assets/basic_example.png" alt="basic" /></p>

<p>Now, notice what happens to the shape of the likelihood function as $n$ becomes larger.</p>

<p><img src="/assets/tiled_example.png" alt="tiled" /></p>

<p>The left peak gets closer and closer to $0$, and eventually (although you can‚Äôt see it here) it gets arbitrarily close to $0$. That is, it essentially disappears in the limit.</p>

<p>Let‚Äôs think about the two quantities we want to measure here: <strong>maximum of the limiting function</strong> and the <strong>limit of the maximum of these functions</strong>.</p>

<p>The <strong>limiting function</strong> will simply be a peak around $\hat{\theta} = 5$, since the left peak disappears in the limit.</p>

<p>However, the <strong>limit of the maximum of these functions</strong> is at $0$ because the maximum is always approaching $0$. So the limit of the maximum likelihood estimator as $n \to \infty$ is $0$, but recall that the true paramter $\theta_0 = 5$.</p>

<p>Thus, in this case, we can see that the log-likelihood will not converge uniformly to the expected log-likelihood, and the MLE will not be consistent.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Here we‚Äôve reviewed some of the sufficient conditions for the MLE to be a consistent estimator. To build intuition, we saw one pathological case where the likelihood function does not converge uniformly to its expectation.</p>

<h2 id="references">References</h2>

<ul>
  <li>Course notes from Matias Cattaneo‚Äôs course ORF524.</li>
  <li><a href="https://www.wikiwand.com/en/Maximum_likelihood_estimation#/Consistency">Wikipedia article on MLE</a></li>
</ul>

:ET