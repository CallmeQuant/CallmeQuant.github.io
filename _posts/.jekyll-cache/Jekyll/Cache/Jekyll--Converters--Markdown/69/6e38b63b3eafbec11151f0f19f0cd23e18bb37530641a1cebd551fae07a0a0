I"Ú^<p>In many types of programming, random seeds are used to make computational results reproducible by generating a known set of random numbers. However, the choice of a random seed can affect results in non-trivial ways.</p>

<p>Here, I‚Äôll cover a discussion around whether the random seed should be treated as a hyperparameter in machine learning. That is, should we ‚Äúoptimize‚Äù the random seed to get the best results possible? This seems like a silly idea upfront ‚Äî trying to optimize something that is intended to be unrelated to the model‚Äôs results ‚Äî but there is already precedent for doing this in some areas of machine learning.</p>

<h2 id="twitter">Twitter</h2>

<p>In most situations, the random seed can be set without much thought. I was amused by this graph generated by Jake VanderPlas showing the most common seed values:</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The frequency of random seeds between 0 and 1000 on github (data from <a href="https://t.co/xwutMzNI2N">https://t.co/xwutMzNI2N</a>) <a href="https://t.co/Zmp7mwMWil">pic.twitter.com/Zmp7mwMWil</a></p>&mdash; Jake VanderPlas (@jakevdp) <a href="https://twitter.com/jakevdp/status/1247742792861757441?ref_src=twsrc%5Etfw">April 8, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>However, in machine learning (and specifically deep learning), it is becoming clear that the choice of random seed can have a large effect on one‚Äôs code and one‚Äôs ultimate computational results. There has been a debate about how exactly we should think of the random seed: should we treat it as a legitimate hyperparameter, or should we be skeptical of results that depend on the random seed?</p>

<p>I first learned about this debate from a tweet by Prof. Dan Roy (where SOTA means ‚Äústate of the art‚Äù):</p>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Your SOTA code may only be SOTA for some random seeds. Nonsense or new reality? I suppose there are trivial ways to close the gap using restarts and validation data.<a href="https://t.co/nhuJrOlqgs">https://t.co/nhuJrOlqgs</a> <a href="https://t.co/mzRGLGH4ZV">pic.twitter.com/mzRGLGH4ZV</a></p>&mdash; Daniel Roy (@roydanroy) <a href="https://twitter.com/roydanroy/status/1250426849902989317?ref_src=twsrc%5Etfw">April 15, 2020</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>In <a href="https://arxiv.org/abs/2002.06305">the paper</a> Roy referenced, the authors tried to quantify how much their results changed on an NLP task depending on the choice of the initial random seed. The first line of the abstract is</p>

<blockquote>
  <p>Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results.</p>
</blockquote>

<p>In their results, they found that the random seed does indeed affect their results, often in very significant ways:</p>

<blockquote>
  <p>Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored.</p>
</blockquote>

<p>So, if the random seed can have a large effect on downstream results, should we be tuning the random seed as if it were a hyperparameter, and report these seeds and their generators?</p>

<h2 id="background-on-random-seeds">Background on random seeds</h2>

<p>Computers don‚Äôt generate truly random numbers. Instead, they generate <em>pseudo</em>random numbers using pseudorandom number generators. Different programming languages and computer architectures have varying types of these generators, but the basic idea is the same: the user gives the generator a ‚Äúrandom seed‚Äù (which is just a number), and given that seed, the generator generates out a sequence of numbers that follow some probability distribution.</p>

<p>However, given a random seed, this sequence of numbers is always the same. In a way, once the seed is set, the numbers are no longer ‚Äúrandom‚Äù in some sense. For example, consider numpy‚Äôs pseudorandom generator. If we set the seed to be 1, generate 5 numbers, then reset the seed, and generate another 5 numbers, this sequence of numbers will be the same each time.</p>

<p>You can verify this for yourself with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Trial 1
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>

<span class="c1"># Trial 2
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">()</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)])</span>

<span class="c1"># Check if same
</span><span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span> <span class="c1"># should return True
</span></code></pre></div></div>

<h2 id="mechanisms-for-the-random-seeds-influence">Mechanisms for the random seed‚Äôs influence</h2>

<p>It seems clear empirically that the random seed can significantly affect model performance. But <em>how</em> exactly does the random seed influence the results?</p>

<p>In essence, any piece of code that is dependent on the random seed is subject to possibly changing the model‚Äôs performance. The main attributes that are typically determined by random seeds are</p>

<ul>
  <li>Initial values of the model‚Äôs parameters before training</li>
  <li>Which data samples appear in the training set vs. the test set</li>
  <li>The order in which the training samples are presented to the model</li>
</ul>

<p>These are also the attributes studied by Dodge et al.</p>

<p>We‚Äôll step through these one-by-one to show a more detailed picture of how the random seed affects each of these, which in turn affects model performance.</p>

<h2 id="initial-values-of-model-parameters">Initial values of model parameters</h2>

<p>In many learning settings that use gradient descent ‚Äì or another type of iterative learning ‚Äì it‚Äôs common to set the initial values of the parameters randomly. These initial values are directly determined by the random seed.</p>

<p>For example, suppose we have the following objective function that we‚Äôd like to minimize:</p>

\[f(x) = 0.01x^6 + (x-0.2)^3 + 2(x-1)^2.\]

<p>Here‚Äôs a plot of the function:</p>

<p><img src="/assets/f_nonconvex.png" alt="f_nonconvex" /></p>

<p>It has two local minima. Now, say our ultimate goal is to find the <em>global</em> minimum (the one on the right) using gradient descent. Clearly, our initial starting value before starting gradient descent will have a large influence on which minimum we end up in.</p>

<p>Indeed, let‚Äôs try running gradient descent with two different initial random seeds ‚Äî 8 and 9 ‚Äî with the following code. (Here, we assume we have perfect gradient information, which is unrealistic in most situations, but it works for illustrative purposes.)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">6</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">.</span><span class="mi">06</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">4</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1">## RANDOM SEED = 8
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># Get initial value
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">est</span> <span class="o">=</span> <span class="n">x0</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Gradient descent
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">est</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">est</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"f(x)"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"np.random.seed(8)"</span><span class="p">)</span>

<span class="c1"># Plot initial value and final value
</span><span class="n">col</span> <span class="o">=</span> <span class="s">'red'</span> <span class="k">if</span> <span class="n">est</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="s">'blue'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">est</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>

<span class="c1">## RANDOM SEED = 9
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">9</span><span class="p">)</span>

<span class="c1"># Get initial value
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">est</span> <span class="o">=</span> <span class="n">x0</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Gradient descent
</span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">est</span> <span class="o">-=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">est</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s">'black'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"x"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"f(x)"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"np.random.seed(9)"</span><span class="p">)</span>

<span class="c1"># Plot initial value and final value
</span><span class="n">col</span> <span class="o">=</span> <span class="s">'red'</span> <span class="k">if</span> <span class="n">est</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="s">'blue'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">est</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
</code></pre></div></div>

<p>We see that these seeds end up in different minima, with 8 finding the global minimum, and 9 getting stuck in the other local minimum. Here‚Äôs a plot showing each initial value as the tick mark at the bottom, and the final value after gradient descent as a dot on the function plot.</p>

<p><img src="/assets/initial_val_diff_seeds.png" alt="initial_val_diff_seeds" /></p>

<h2 id="traintest-split">Train/test split</h2>

<p>The way that the random seed affects model performance via the split between train and test data is a bit more straightforward. At its core, it depends on whether the data in the train split is representative of the whole population of data. If you fit a model on an outlier subset of data, then the model will likely perform poorly on the test data.</p>

<p>To make this more concrete, let‚Äôs consider the setting of simple linear regression. Let‚Äôs say we have 100 total data points, and we fit the model on 75 of them. We can choose these 75 points randomly, by first setting a random seed.</p>

<p>Let‚Äôs say we fit the linear model 10 times, using random seeds 1 through 10. This can be done with the following code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>

<span class="n">train_frac</span> <span class="o">=</span> <span class="mf">0.75</span>
<span class="n">n_train</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_frac</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>

<span class="n">r2s</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
    
    <span class="c1"># Set seed
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">ii</span><span class="p">)</span>
    
    <span class="c1"># Train/test split
</span>    <span class="n">train_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">n_train</span><span class="p">)</span>
    <span class="n">test_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">train_idx</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
    <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        
    <span class="c1"># Estimate beta and test
</span>    <span class="n">ols_est</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
    <span class="n">yhat_test</span> <span class="o">=</span> <span class="n">ols_est</span> <span class="o">*</span> <span class="n">X_test</span>
    <span class="n">r2s</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">r2_score</span><span class="p">(</span><span class="n">yhat_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<p>We can see that the $R^2$ of the fit on the test data varies fairly substantially depending on the random seed.</p>

<p><img src="/assets/train_test_split_r2.png" alt="train_test_split_r2" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>It‚Äôs undeniable that the random seed has a substantive effect on model results in many learning situations. However, it‚Äôs up for debate how we should think of random seeds, and if/how we should report random seeds in the literature.</p>

<p>Some models‚Äô dependence on randomness has been known for quite a while (e.g., doing random restarts with certain algorithms, like k-means). However, if certain random seeds are known to perform better with some types of algorithms on certain datasets, this seems worth reporting in a consistent manner. This is even more true when fitting these models is expensive in terms of time, money, and electricity (this is particularly true in <a href="https://twitter.com/eturner303/status/1143174828804857856">deep learning</a>).</p>

<p>The random seed debate could be another point in favor of <a href="http://www.argmin.net/2017/12/05/kitchen-sinks/">deep learning being alchemy</a>, or a simple reality about algorithms that depend on randomness. Either way, the empirical effect of random seeds is clear.</p>

<h2 id="references">References</h2>

<ul>
  <li>Dodge, Jesse, et al. ‚ÄúFine-tuning pretrained language models: Weight initializations, data orders, and early stopping.‚Äù arXiv preprint arXiv:2002.06305 (2020).</li>
</ul>

:ET