I"ä<p>As their name suggests, ‚Äúquasi-likelihoods‚Äù are quantities that aren‚Äôt formally likelihood functions, but can be used as replacements for formal likelihoods in more general settings.</p>

<p>A major reason that quasi-likelihoods are useful is that they don‚Äôt require the statistician to specify a full probability distribution for the data ‚Äì instead, only the first and second moments must be specified. This allows the modeler to have more flexibility without having to impose unnecessary (and incorrect) assumptions.</p>

<p>Most results for quasi-likelihoods are in the context of the exponential family, which we review here first.</p>

<h2 id="exponential-family">Exponential family</h2>

<p>Recall the form of the exponential family of distributions:</p>

\[f(y; \theta) = \exp\left\{ a(y) b(\theta) + c(\theta) + d(y) \right\}.\]

<p>for some functions $a$, $b$, $c$, and $d$. In canonical form, we have $a(y) = y$ and $b(\theta) = \theta$. The form of the likelihood then simplifies to</p>

\[f(y; \theta) = \exp\left\{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right\}\]

<p>where we call $\phi$ a ‚Äúdispersion parameter‚Äù.</p>

<h2 id="likelihood-for-glms">Likelihood for GLMs</h2>

<p>Generalized linear models (GLMs) extend the classical linear model to allow for various types of response distributions. For a response vector $\mathbf{Y}$, a matrix of covariates $\mathbf{X}$, and a vector of coefficients $\boldsymbol{\beta}$, a GLM has the model structure of</p>

\[\mathbb{E}[\mathbf{Y} | \mathbf{X}] = \mu(\mathbf{X}) = g^{-1}(\mathbf{X} \boldsymbol{\beta})\]

<p>where $g(\cdot)$ is a link function. Equivalently, we have $g(\mu(\mathbf{X})) = \mathbf{X}\boldsymbol{\beta}$, which we typically define to be the parameter $\theta$: $\theta = g(\mu(\mathbf{X})) = g(\mu)$. We also define $\eta$ to be the linear predictor: $\eta = \mathbf{X} \boldsymbol{\beta}$.</p>

<p>Additionally, $\mathbf{Y} | \mathbf{X}$ is typically assumed to come from an exponential family distribution. For a sample of size $n$, the likelihood is then</p>

\[L(\theta) = \prod\limits_{i=1}^n \exp\left\{ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right\}.\]

<p>The corresponding log-likelihood is</p>

\[\ell(\theta) = \sum\limits_{i=1}^n \left[\frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi)\right].\]

<p>To perform maximum likelihood estimation, we take the derivative w.r.t. the coefficients $\beta_1, \dots, \beta_p$ and set these expressions to zero. This can be accomplished with the chain rule:</p>

\[\frac{\partial \ell_i}{\partial \beta_j} = \frac{\partial \ell_i}{\partial \theta_i} \cdot \frac{\partial \theta_i}{\partial \mu_i} \cdot \frac{\partial \mu_i}{\partial \eta_i} \cdot \frac{\partial \eta_i}{\partial \beta_j}.\]

<p>Breaking these partial derivatives down, we have</p>

<p>\begin{align} \frac{\partial \ell_i}{\partial \theta_i} &amp;= \frac{y_i - b‚Äô(\theta_i)}{a(\phi)} = \frac{y_i - \mu_i}{a(\phi)} \\ \frac{\partial \theta_i}{\partial \mu_i} &amp;= g‚Äô(\mu_i) = (b‚Äô‚Äò(y_i))^{-1} =  \frac{a(\phi)}{\text{Var}(y_i)} = \frac{1}{v(y_i)} \\ \frac{\partial \eta_i}{\partial \beta_j} &amp;= X_{ij} \\ \end{align}</p>

<p>The expression for $\frac{\partial \ell_i}{\partial \theta_i}$ follows because $b‚Äô(\theta)$ is equal to the mean $\mu$ in the exponential family (see <a href="https://andrewcharlesjones.github.io/posts/2020/03/generalizedpca/">here</a> for an explanation). We won‚Äôt specify $\frac{\partial \mu_i}{\partial \eta_j}$ further, just to allow for an arbitrary relationship between $\mu$ and $\eta$.</p>

<p>Putting these together, we have</p>

<p>\begin{align} \frac{\partial \ell_i}{\partial \beta_j} &amp;= \frac{y_i - \mu_i}{a(\phi)} \cdot \frac{1}{v(y_i)} \cdot \frac{\partial \mu_i}{\partial \eta_j} \cdot X_{ij} \\ &amp;= \frac{(y_i - \mu_i) X_{ij}}{a(\phi) v(\mu_i)} \cdot \frac{\partial \mu_i}{\partial \eta_j} \end{align}</p>

<p>Setting these equations to zero gives us the score equations needed to perform MLE:</p>

\[\sum\limits_{i=1}^n \frac{(y_i - \mu_i) X_{ij}}{a(\phi) v(\mu_i)} \cdot \frac{\partial \mu_i}{\partial \eta_j} = 0, \;\;\; j = 1, \dots, p\]

<p>Notice that these equations depend on the specified probability distribution through $\mu_i$ and the variance function $v(\mu)$. Specifically, once a distribution is specified, this variance function is automatically specified as well.</p>

<p>We now turn to quasi-likelihoods, which allow us to avoid specifying an entire distribution. Instead, we directly specify the mean-variance relationship.</p>

<h2 id="quasi-likelihoods">Quasi-likelihoods</h2>

<p>A quasi-likelihood is defined as</p>

\[\int_y^\mu \frac{y - t}{a(\phi) V(t)} dt\]

<p>where $V$ is a function that relates the variance to the mean, and $a(\phi)$ is a (unknown) dispersion parameter. Differentiating, we have</p>

\[\frac{\partial Q}{\partial \mu} = \frac{y - \mu}{a(\phi) V(\mu)}\]

<p>We can see already that this expression is equivalent to the typical likelihood case, $\frac{\partial \ell}{\partial \mu}$.</p>

<p>To get the full ‚Äúquasi-score equations‚Äù, we have to perform one more step of the chain rule, by multiplying by $\frac{\partial \mu}{\partial \beta}$:</p>

\[\sum\limits_{i=1}^n \frac{(y_i - \mu_i)}{a(\phi) v(\mu_i)} \cdot \frac{\partial \mu_i}{\partial \beta_j} = 0, \;\;\; j = 1, \dots, p\]

<p>Notice that the coefficient estimates $\hat{\beta}$ are the same for the full GLM case and for quasi-likelihoods. However, they will have different variance estimates.</p>

<h2 id="quasi-likelihoods-corresponding-to-well-known-likelihoods">Quasi-likelihoods corresponding to well-known likelihoods</h2>

<p>To build intuition, it‚Äôs useful to note that many well-known distributions correspond to a particular choice of the mean-variance relationship in the world of quasi-likelihood world. The converse is not true: there are many quasi-likelihood specifications that do not map onto any known distributions.</p>

<p>Table 9.1 from Peter McCullagh‚Äôs book Generalized Linear Models shows several such examples:</p>

<p><img src="/assets/glmbook_ql_table.png" alt="glmbook_ql_table" /></p>

<p>The column $Q(\mu; y)$ here is calculated by computing the integral</p>

\[\int_y^\mu \frac{y - t}{a(\phi) V(t)} dt\]

<p>for the specified function $V(\mu)$, and it is assumed $a(\phi) = 1$ for simplicity.</p>

<p>For example, in the Gaussian case we have $V(\mu) = 1$, which implies</p>

<p>\begin{align} \int_y^\mu \frac{y - t}{a(\phi) V(t)} dt &amp;= \int_y^\mu \frac{y - t}{1 \cdot 1} dt \\ &amp;= \int_y^\mu (y - t) dt \\ &amp;= -\frac12 (y - t)^2 \big\rvert_y^\mu \\ &amp;= -\frac12 (y - \mu)^2 \end{align}</p>

<p>In the Poisson case, we have $V(\mu) = \mu$, which implies</p>

<p>\begin{align} \int_y^\mu \frac{y - t}{a(\phi) V(t)} dt &amp;= \int_y^\mu \frac{y - t}{1 \cdot t} dt \\ &amp;= \int_y^\mu \left(\frac{y}{t} - 1\right) dt \\ &amp;= y \log \mu - \mu \\ \end{align}</p>

<p>However, notice that the table also includes variance function with no corresponding closed-form likelihood. An interesting case of this is when $V(\mu) = \mu^\zeta$, where $\zeta \neq 0, 1, 2$. This power law mean-variance relationship occurs often in ecology through <a href="https://www.wikiwand.com/en/Taylor%27s_law">Taylor‚Äôs law</a>, and more generally in statistics it appears in the family of distributions known as the <a href="https://www.wikiwand.com/en/Tweedie_distribution#">Tweedie distribution</a>.</p>

<h2 id="references">References</h2>

<ul>
  <li>McCullagh, Peter. Generalized linear models. Routledge, 2018.</li>
  <li>Agresti, Alan. Foundations of linear and generalized linear models. John Wiley &amp; Sons, 2015.</li>
  <li>Islam, M. Ataharul, and Rafiqul I. Chowdhury. Analysis of repeated measures data. Singapore: Springer, 2017.</li>
</ul>
:ET