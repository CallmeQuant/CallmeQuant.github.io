I"U<p>The power iteration algorithm is a numerical approach to computing the top eigenvector and eigenvalue of a matrix.</p>

<h2 id="background">Background</h2>

<p>Consider a diagonalizable matrix $A \in \mathbb{R}^{n \times n}$ with eigenvalue decomposition</p>

\[A = V \Lambda V^{-1}.\]

<p>In this notation, the columns of $V$ contains the eigenvectors of $A$, and $\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)$ contains the corresponding eigenvalues with $|\lambda_1| &gt; \cdots &gt; |\lambda_n|$.</p>

<p>Consider what happens when we take powers of $A$:</p>

<p>\begin{align} A &amp;= V \Lambda V^{-1} V \\ AA = V \Lambda V^{-1} V \Lambda V^{-1} &amp;= V \Lambda^2 V^{-1} \\ AAA = V \Lambda V^{-1} V \Lambda V^{-1} V \Lambda V^{-1} &amp;= V \Lambda^3 V^{-1} \\ &amp;\vdots \\ \underbrace{A \cdots A}_{\text{$k$ times}} &amp;= V \Lambda^k V^{-1}. \\ \end{align}</p>

<p>In the above, powers of $\Lambda$ are equivalent to $\Lambda^k = \text{diag}(\lambda_1^k, \dots, \lambda_n^k)$.</p>

<h2 id="method">Method</h2>

<p>Consider a random vector $\mathbf{b} \in \mathbb{R}^n$. Note that $\mathbf{b}$ can always be written as a linear combination of $A$’s eigenvectors:</p>

\[b = \sum\limits_{i=1}^n \mathbf{v}_i b_i = V\widetilde{\mathbf{b}}\]

<p>where $\widetilde{\mathbf{b}} = (\widetilde{b}_1, \dots, \widetilde{b}_n)^\top$ is another vector.</p>

<p>Now, let’s rewrite $A^k \mathbf{b} = A^k V \widetilde{\mathbf{b}}$ as a sum over eigenvalues and eigenvectors:</p>

\[A^k V \widetilde{\mathbf{b}} = V \Lambda^k V^{-1} V \widetilde{\mathbf{b}} = V \Lambda^k \widetilde{\mathbf{b}} = \sum\limits_{i=1}^n \mathbf{v}_i \lambda_i^k \widetilde{b}_i.\]

<p>Now, to understand how the top eigenvalue relates to the others, let’s pull out $\lambda_1$:</p>

<p>\begin{equation} \label{eq:eq1}
    A^k V \widetilde{\mathbf{b}} = \lambda_1^k \sum\limits_{i=1}^n \mathbf{v}_i \left(\frac{\lambda_i}{\lambda_1}\right)^k \widetilde{b}_i.
\end{equation}</p>

<p>With each power of $A$, the value of $\left(\frac{\lambda_i}{\lambda_1}\right)^k$ will decrease, eventually going to zero as $k \to \infty$. In other words, as $k\to \infty$,</p>

\[A^k V \widetilde{\mathbf{b}} \to \mathbf{v}_1 \lambda_1^k b_1.\]

<p>This means that $A^k V \widetilde{\mathbf{b}} = A^k \mathbf{b}$ will converge to a vector that is a scalar multiple of the top eigenvector (multiplied by $\lambda_1^k b_1$). Thus, all that’s left to do is normalize this expression to make it a unit vector, and we’re left with</p>

\[\frac{A^k \mathbf{b}}{\|A^k \mathbf{b}\|_2}.\]

<p>Putting it all together, we have the full power method:</p>

<ol>
  <li>Draw random $n$-vector $\mathbf{b}_0$.</li>
  <li>Set $\mathbf{b}_1 := A \mathbf{b}_0$.</li>
  <li>For $k=2, \dots, K$:
3.1. Set $\widehat{\mathbf{b}}_k := A \mathbf{b}_{k-1}$.
3.2 Normalize by setting $\mathbf{b}_k = \frac{\widehat{\mathbf{b}}_k}{||\widehat{\mathbf{b}}_k||_2}$.</li>
  <li>Return $\mathbf{b}_K$.</li>
</ol>

<h2 id="experiments">Experiments</h2>

<p>The power iteration method can be implemented fairly easily in Python. Below is some sample code for finding the top eigenvector for a given matrix <code class="language-plaintext highlighter-rouge">A</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_iters</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Starting vector
</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Power iteration
</span><span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iters</span><span class="p">):</span>
    
    <span class="c1"># Project
</span>    <span class="n">bnew</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">b</span>
    
    <span class="c1"># Normalize
</span>    <span class="n">b</span> <span class="o">=</span> <span class="n">bnew</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">bnew</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Testing this on a random $10 \times 10$ matrix, we can see that the estimated top eigenvector quickly approaches the true top eigenvector. Below, we plot each element of the eigenvector as a point.</p>

<p><img src="/assets/power_iteration_plots_timelapse1.png" alt="power_iteration_plots_timelapse1" /></p>

<p>We can view this convergence another way by estimating the mean-squared error (MSE) at each iteration.</p>

<p><img src="/assets/power_iteration_error.png" alt="power_iteration_error" /></p>

<p>However, it’s important to note that the rate of convergence of this method will depend on a few factors. One of the most important is how tightly “spiked” the top eigenvalue is. In particular, if $|\lambda_1| \gg |\lambda_2|$, then it will quickly converge. However, if these are close to one another, it will take longer. Intuitively, this is because the ratio $\frac{\lambda_2}{\lambda_1}$ in \eqref{eq:eq1} will take longer to go to zero.</p>

<p>We can show this phenomenon numerically by fixing the eigenvectors of a matrix and varying the difference between $\lambda_1$ and $\lambda_2$. Below, we plot the error over iterations, with each line representing a different set of eigenvalues. The legend shows the value $|\lambda_1| - |\lambda_2|$.</p>

<p><img src="/assets/power_iteration_diff_eigvals.png" alt="power_iteration_diff_eigvals" /></p>

<p>Clearly, as the top eigenvalue becomes more dominant, the power iteration method converges faster.</p>

<h2 id="references">References</h2>

<ul>
  <li>Prof. David Bindel’s <a href="https://www.cs.cornell.edu/~bindel/class/cs6210-f16/lec/2016-10-17.pdf">notes</a> on the power iteration method.</li>
  <li><a href="https://www.wikiwand.com/en/Power_iteration">Wikipedia page</a> on the power iteration method.</li>
</ul>
:ET